{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b52bb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymorphy2 in /Users/anna/opt/anaconda3/lib/python3.8/site-packages (0.9.1)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /Users/anna/opt/anaconda3/lib/python3.8/site-packages (from pymorphy2) (2.4.417127.4579844)\n",
      "Requirement already satisfied: docopt>=0.6 in /Users/anna/opt/anaconda3/lib/python3.8/site-packages (from pymorphy2) (0.6.2)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /Users/anna/opt/anaconda3/lib/python3.8/site-packages (from pymorphy2) (0.7.2)\n",
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20.0 in /Users/anna/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.20.1)\n",
      "Requirement already satisfied: numexpr in /Users/anna/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (2.7.3)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: gensim in /Users/anna/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (4.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/anna/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (52.0.0.post20210125)\n",
      "Requirement already satisfied: joblib in /Users/anna/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.0.1)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /Users/anna/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.2.4)\n",
      "Requirement already satisfied: future in /Users/anna/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/anna/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (0.24.1)\n",
      "Collecting funcy\n",
      "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/anna/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (2.11.3)\n",
      "Requirement already satisfied: scipy in /Users/anna/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/anna/opt/anaconda3/lib/python3.8/site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/anna/opt/anaconda3/lib/python3.8/site-packages (from pandas>=1.2.0->pyLDAvis) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/anna/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/anna/opt/anaconda3/lib/python3.8/site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/anna/opt/anaconda3/lib/python3.8/site-packages (from jinja2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/anna/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->pyLDAvis) (2.1.0)\n",
      "Building wheels for collected packages: pyLDAvis, sklearn\n",
      "  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136898 sha256=da2f44270172b16d3392d9c9f00a4bd3a817b70a652cce438475b449d1ca1c60\n",
      "  Stored in directory: /Users/anna/Library/Caches/pip/wheels/90/61/ec/9dbe9efc3acf9c4e37ba70fbbcc3f3a0ebd121060aa593181a\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=73f21fd96d808f37fee7ecb39e4aa7a276fc3d012a087efe4535cb0b00833099\n",
      "  Stored in directory: /Users/anna/Library/Caches/pip/wheels/22/0b/40/fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
      "Successfully built pyLDAvis sklearn\n",
      "Installing collected packages: sklearn, funcy, pyLDAvis\n",
      "Successfully installed funcy-1.17 pyLDAvis-3.3.1 sklearn-0.0\n",
      "Requirement already satisfied: razdel in /Users/anna/opt/anaconda3/lib/python3.8/site-packages (0.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anna/opt/anaconda3/lib/python3.8/site-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n",
      "/Users/anna/opt/anaconda3/lib/python3.8/site-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n",
      "/Users/anna/opt/anaconda3/lib/python3.8/site-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n",
      "/Users/anna/opt/anaconda3/lib/python3.8/site-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n",
      "/Users/anna/opt/anaconda3/lib/python3.8/site-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n",
      "/Users/anna/opt/anaconda3/lib/python3.8/site-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n",
      "/Users/anna/opt/anaconda3/lib/python3.8/site-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n",
      "/Users/anna/opt/anaconda3/lib/python3.8/site-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n",
      "/Users/anna/opt/anaconda3/lib/python3.8/site-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n",
      "/Users/anna/opt/anaconda3/lib/python3.8/site-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n",
      "/Users/anna/opt/anaconda3/lib/python3.8/site-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n",
      "/Users/anna/opt/anaconda3/lib/python3.8/site-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2\n",
    "!pip install pyLDAvis\n",
    "!pip install razdel\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import pyLDAvis.gensim_models\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA, LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import warnings\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "morph = MorphAnalyzer()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2964a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09d5f463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавим лемматизацию\n",
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    normalized_text = [morph.parse(word)[0].normal_form for word in normalized_text]\n",
    "    return ' '.join(normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f79d5aa",
   "metadata": {},
   "source": [
    "Домашнее задание № 5. Матричные разложения/Тематическое моделирование\n",
    "Задание № 1 (4 балла)\n",
    "Попробуйте матричные разложения с 5 классификаторами - SGDClassifier, KNeighborsClassifier, RandomForest, ExtraTreesClassifier (про него подробнее почитайте в документации, он похож на RF). Используйте и NMF и SVD. Сравните результаты на кросс-валидации и выберите лучшее сочетание.\n",
    "\n",
    "В итоге у вас должно получиться, как минимум 10 моделей (два разложения на каждый классификатор). Используйте 1 и те же параметры кросс-валидации. Параметры векторизации, параметры K в матричных разложениях, параметры классификаторов могут быть разными между экспериментами.\n",
    "Можете взять поменьше данных, если все будет обучаться слишком долго (не ставьте параметр K слишком большим в NMF, иначе точно будет слишком долго)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4692cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('avito_category_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb9367b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['description_norm'] = data['description'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fd77ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_nmf_sgdc = Pipeline([\n",
    "    ('bow', CountVectorizer(tokenizer=lambda x: x.split(), min_df=3, max_df=0.3)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('decomposition', NMF(40)),\n",
    "    ('clf', SGDClassifier(max_iter=1000, tol=1e-3))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "92729cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_nmf_kn = Pipeline([\n",
    "    ('bow', CountVectorizer(tokenizer=lambda x: x.split(), min_df=3, max_df=0.3)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('decomposition', NMF(40)),\n",
    "    ('clf', KNeighborsClassifier(n_neighbors=3))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "48f6b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_nmf_et = Pipeline([\n",
    "    ('bow', CountVectorizer(tokenizer=lambda x: x.split(), min_df=3, max_df=0.3)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('decomposition', NMF(100)),\n",
    "    ('clf', ExtraTreesClassifier(n_estimators=100))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fb45dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_svd_sgdc = Pipeline([\n",
    "    ('bow', CountVectorizer(tokenizer=lambda x: x.split(), ngram_range=(1,2), min_df=5, max_df=0.4)),\n",
    "    ('svd', TruncatedSVD(100)),\n",
    "    ('clf', SGDClassifier(max_iter=1000, tol=1e-3))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "437efb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_svd_rf = Pipeline([\n",
    "    ('bow', CountVectorizer(tokenizer=lambda x: x.split(), ngram_range=(1,2), min_df=5, max_df=0.4)),\n",
    "    ('svd', TruncatedSVD(100)),\n",
    "    ('clf', RandomForestClassifier(n_estimators=100, max_depth=10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3f52adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_svd_kn = Pipeline([\n",
    "    ('bow', CountVectorizer(tokenizer=lambda x: x.split(), ngram_range=(1,2), min_df=5, max_df=0.4)),\n",
    "    ('svd', TruncatedSVD(100)),\n",
    "    ('clf', KNeighborsClassifier(n_neighbors=3))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4ff742c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_svd_et = Pipeline([\n",
    "    ('bow', CountVectorizer(tokenizer=lambda x: x.split(), ngram_range=(1,2), min_df=5, max_df=0.4)),\n",
    "    ('svd', TruncatedSVD(100)),\n",
    "    ('clf', ExtraTreesClassifier(n_estimators=100))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "28af7d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_table(X, y, pipeline, N=6):\n",
    "    # зафиксируем порядок классов\n",
    "    labels = list(set(y))\n",
    "    \n",
    "    # метрики отдельных фолдов будет хранить в табличке\n",
    "    fold_metrics = pd.DataFrame(index=labels)\n",
    "    # дополнительно также соберем таблицу ошибок\n",
    "    errors = np.zeros((len(labels), len(labels)))\n",
    "    \n",
    "    # создаем стратегию кросс-валидации\n",
    "    # shuffle=True (перемешивание) - часто критично важно указать\n",
    "    # т.к. данные могут быть упорядочены и модель на этом обучится\n",
    "    kfold = StratifiedKFold(n_splits=N, shuffle=True, )\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(X, y)):\n",
    "        # fit-predict как и раньше, но сразу пайплайном\n",
    "        pipeline.fit(X[train_index], y[train_index])\n",
    "        preds = pipeline.predict(X[test_index])\n",
    "        \n",
    "        # записываем метрику и индекс фолда\n",
    "        fold_metrics[f'precision_{i}'] = precision_score(y[test_index], preds, labels=labels, average=None)\n",
    "        fold_metrics[f'recall_{i}'] = recall_score(y[test_index], preds, labels=labels, average=None)\n",
    "        fold_metrics[f'f1_{i}'] = f1_score(y[test_index], preds, labels=labels, average=None)\n",
    "        errors += confusion_matrix(y[test_index], preds, labels=labels, normalize='true')\n",
    "    \n",
    "    # таблица для усредненных значений\n",
    "    # тут мы берем колонки со значениями и усредняем их\n",
    "    # часто также все метрики сразу суммируют и в конце просто делят на количество фолдов\n",
    "    # но мы тут помимо среднего также хотим посмотреть на стандартное отклонение\n",
    "    # чтобы понять как сильно варьируются оценки моделей\n",
    "    result = pd.DataFrame(index=labels)\n",
    "    result['precision'] = fold_metrics[[f'precision_{i}' for i in range(N)]].mean(axis=1).round(2)\n",
    "    result['precision_std'] = fold_metrics[[f'precision_{i}' for i in range(N)]].std(axis=1).round(2)\n",
    "    \n",
    "    result['recall'] = fold_metrics[[f'recall_{i}' for i in range(N)]].mean(axis=1).round(2)\n",
    "    result['recall_std'] = fold_metrics[[f'recall_{i}' for i in range(N)]].std(axis=1).round(2)\n",
    "    \n",
    "    result['f1'] = fold_metrics[[f'f1_{i}' for i in range(N)]].mean(axis=1).round(2)\n",
    "    result['f1_std'] = fold_metrics[[f'f1_{i}' for i in range(N)]].std(axis=1).round(2)\n",
    "    \n",
    "    # добавим одну колонку со средним по всем классам\n",
    "    result.loc['mean'] = result.mean().round(2)\n",
    "    # проценты ошибок просто усредняем\n",
    "    errors /= N\n",
    "    \n",
    "    return result, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2f8970",
   "metadata": {},
   "source": [
    "Теперь куча метрик:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "77922f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_nmf_sgd, errors_nmf_sgd = eval_table(data['description_norm'], data['category_name'], pipeline_nmf_sgdc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4a1857a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_nmf_rf, errors_nmf_rf = eval_table(data['description_norm'], data['category_name'], pipeline_nmf_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cfafbc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_nmf_kn, errors_nmf_kn = eval_table(data['description_norm'], data['category_name'], pipeline_nmf_kn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d034af7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_nmf_et, errors_nmf_et = eval_table(data['description_norm'], data['category_name'], pipeline_nmf_et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "39db330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_svd_sgdc, errors_svd_sgdc = eval_table(data['description_norm'], data['category_name'], pipeline_svd_sgdc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "965b36d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_svd_rf, errors_svd_rf = eval_table(data['description_norm'], data['category_name'], pipeline_svd_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "be3c69a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_svd_kn, errors_svd_kn = eval_table(data['description_norm'], data['category_name'], pipeline_svd_kn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "65e07a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_svd_et, errors_svd_et = eval_table(data['description_norm'], data['category_name'], pipeline_svd_et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e48a5eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(columns=metrics_svd_rf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5fb67366",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = metrics.append(metrics_nmf_sgdc.loc['mean'])\n",
    "metrics = metrics.append(metrics_nmf_rf.loc['mean'])\n",
    "metrics = metrics.append(metrics_nmf_kn.loc['mean'])\n",
    "metrics = metrics.append(metrics_nmf_et.loc['mean'])\n",
    "metrics = metrics.append(metrics_svd_sgdc.loc['mean'])\n",
    "metrics = metrics.append(metrics_svd_rf.loc['mean'])\n",
    "metrics = metrics.append(metrics_svd_kn.loc['mean'])\n",
    "metrics = metrics.append(metrics_svd_et.loc['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e5ac8594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      precision  precision_std  recall  recall_std    f1  f1_std\n",
      "mean       0.62           0.18    0.49        0.12  0.47    0.09\n",
      "mean       0.75           0.12    0.46        0.06  0.48    0.06\n",
      "mean       0.50           0.04    0.49        0.04  0.48    0.03\n",
      "mean       0.74           0.04    0.68        0.04  0.70    0.03\n",
      "mean       0.70           0.08    0.64        0.06  0.66    0.03\n",
      "mean       0.72           0.08    0.46        0.03  0.49    0.03\n",
      "mean       0.52           0.04    0.44        0.04  0.45    0.03\n",
      "mean       0.65           0.06    0.48        0.04  0.52    0.04\n"
     ]
    }
   ],
   "source": [
    "print (metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb55d12",
   "metadata": {},
   "source": [
    "Видно, что у NMF-RandomForest и NMF-ExtraTreesClassifier самые лучшие показатели."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
